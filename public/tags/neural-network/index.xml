<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Network on </title>
    <link>https://coosis.github.io/tags/neural-network/</link>
    <description>Recent content in Neural Network on </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>coosislee@gmail.com (Coosis)</managingEditor>
    <webMaster>coosislee@gmail.com (Coosis)</webMaster>
    <lastBuildDate>Sun, 03 Dec 2023 09:37:35 +0800</lastBuildDate>
    <atom:link href="https://coosis.github.io/tags/neural-network/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>First-Time: Neural Network(overview)</title>
      <link>https://coosis.github.io/posts/firsttime_neuralnetwork/</link>
      <pubDate>Sun, 03 Dec 2023 09:37:35 +0800</pubDate><author>coosislee@gmail.com (Coosis)</author>
      <guid>https://coosis.github.io/posts/firsttime_neuralnetwork/</guid>
      <description>&lt;h1 id=&#34;before-everything-else&#34;&gt;Before Everything Else:&lt;/h1&gt;&#xA;&lt;p&gt;So one day, out of no where, I got thinking: How does a language model generate texts? I searched, and found a &lt;a href=&#34;https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&#34;&gt;neural network course&lt;/a&gt; by &lt;a href=&#34;https://karpathy.ai/&#34;&gt;Andrej Karpathy&lt;/a&gt;. Immediately I dived in.&lt;/p&gt;&#xA;&lt;h1 id=&#34;structurehistory&#34;&gt;Structure/History:&lt;/h1&gt;&#xA;&lt;p&gt;The different stages of neural networks are the following:&#xA;Bigram (one character predicts the next one with a lookup table of counts)&#xA;MLP, following &lt;a href=&#34;https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&#34;&gt;Bengio et al. 2003&lt;/a&gt;&#xA;CNN, following &lt;a href=&#34;https://arxiv.org/abs/1609.03499&#34;&gt;DeepMind WaveNet 2016&lt;/a&gt;&#xA;RNN, following &lt;a href=&#34;https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf&#34;&gt;Mikolov et al. 2010&lt;/a&gt;&#xA;LSTM, following &lt;a href=&#34;https://arxiv.org/abs/1308.0850&#34;&gt;Graves et al. 2014&lt;/a&gt;&#xA;GRU, following &lt;a href=&#34;https://arxiv.org/abs/1409.1259&#34;&gt;Kyunghyun Cho et al. 2014&lt;/a&gt;&#xA;Transformer, following &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Vaswani et al. 2017&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
